<configuration>
    <property resource="application.properties"/>
    <property resource="application-${spring.profiles.active}.properties"/>

    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%white(%d{yyyy-MM-dd HH:mm:ss}) %highlight([%thread]) %green(%-5level) %cyan(%logger{10}) : %white(%msg%n)</pattern>
            <!--<pattern>%d{HH:mm:ss.SSS} %-5level %logger{36}:%line - %msg%n</pattern>-->
        </encoder>
    </appender>

    <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %logger{36}:%line - %m%n</pattern>
        </encoder>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>./logs/info/wwb-%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>30</maxHistory>
            <totalSizeCap>10GB</totalSizeCap>
        </rollingPolicy>
    </appender>

    <!--按天生成error日志-->
    <appender name="errorLog" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!--如果是 true，日志会被安全的写入文件，即使其他的FileAppender也在向此文件做写入操作，效率低，默认是 false。-->
        <Prudent>true</Prudent>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>ERROR</level>
        </filter>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>./logs/error/web-%d{yyyy-MM-dd}.log</FileNamePattern><!--定义了日志的切分方式——把每一天的日志归档到一个文件中-->
            <maxHistory>30</maxHistory><!--只保留最近30天的日志-->
            <totalSizeCap>1GB</totalSizeCap><!--指定日志文件的上限大小，例如设置为1GB的话，那么到了这个值，就会删除旧的日志。-->
        </rollingPolicy>
        <layout class="ch.qos.logback.classic.PatternLayout">
            <Pattern>
                %d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n
            </Pattern>
        </layout>
    </appender>


    <!-- This is the kafkaAppender -->
    <!--	<appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
            <encoder class="com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder">
                <layout class="ch.qos.logback.classic.PatternLayout">
                    <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} %-5level %logger{36}:%line - %m%n</pattern>
                </layout>
            </encoder>
            <topic>logs</topic>
            <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.RoundRobinKeyingStrategy" />
            <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />
            <producerConfig>bootstrap.servers=${spring.kafka.bootstrap-servers}</producerConfig>
            <appender-ref ref="STDOUT" />
        </appender>-->


    <root level="INFO">
        <appender-ref ref="STDOUT"/>
        <!--<appender-ref ref="kafkaAppender" />-->
        <!--<appender-ref ref="FILE"/>-->
        <appender-ref ref="errorLog"/><!--输出error日志文件-->
    </root>
    <!--<logger name="org.apache.kafka" level="ERROR" />-->
    <logger name="org.apache.curator" level="ERROR"/>
    <logger name="org.springframework" level="ERROR"/>
    <logger name="org.apache.commons" level="ERROR"/>
    <logger name="com.xdaocloud.futurechain" level="DEBUG"/>

    <!--myibatis log configure -->
    <logger name="com.apache.ibatis" level="TRACE"/>
    <logger name="java.sql.Connection" level="DEBUG"/>
    <logger name="java.sql.Statement" level="DEBUG"/>
    <logger name="java.sql.PreparedStatement" level="DEBUG"/>
</configuration>